{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a virtual environment and install dependencies\n",
    "#!python -m venv venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "! .\\env\\Scripts\\Activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa0598",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas python-dateutil\n",
    "!pip install kafka-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d40f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pandas as pd\n",
    "from dateutil import tz\n",
    "from dateutil import parser as dateparser\n",
    "from datetime import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab211e",
   "metadata": {},
   "source": [
    "#### Data preprocessing method definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91594a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Util methods\n",
    "\n",
    "# Create the clean column names\n",
    "def cleanColumnNames(df):\n",
    "    df.columns = (df.columns.str.strip().str.lower().str.replace(r\"\\s+\", \"_\", regex=True))\n",
    "    return df\n",
    "\n",
    "\n",
    "def validate_format_date_time_objects(df):\n",
    "    # Convert the date_time into a date time object\n",
    "    df['datetime'] = pd.to_datetime(df['read_date'], errors='coerce')\n",
    "\n",
    "    # localize to America/Chicago, then convert to UTC\n",
    "    s = pd.to_datetime(df['read_date'], errors='coerce')\n",
    "    s = (s.dt.tz_localize('America/Chicago', nonexistent='shift_forward', ambiguous='NaT').dt.tz_convert('UTC'))\n",
    "    df['datetime_utc'] = s\n",
    "    df['timestamp_ms'] = (s.view('int64') // 10**6).astype('Int64')\n",
    "    return df\n",
    "\n",
    "\n",
    "def validate_boolean_data(df):\n",
    "    # Convert the heavy_vehicle column to numeric, coercing errors to NaN\n",
    "    # Use pandas nullable integer dtype so NaN/empty values are preserved instead of raising on astype(int)\n",
    "    df['heavy_vehicle'] = pd.to_numeric(df.get('heavy_vehicle', pd.Series()), errors='coerce').astype('Int64')\n",
    "    return df\n",
    "\n",
    "\n",
    "def check_null_values(df, display_count):\n",
    "    # Show missing value counts based on the column names\n",
    "    print(\"\\n Missing value summary (exact string '',NaN):\")\n",
    "    missing_report = pd.DataFrame({\n",
    "        'missing_count_na': df.isna().sum(),\n",
    "        'empty_count_na': (df == '').sum(),\n",
    "        'unique_values': df.nunique(dropna=False)\n",
    "    }).sort_values('missing_count_na', ascending=False)\n",
    "    display(missing_report.head(display_count))\n",
    "\n",
    "\n",
    "def drop_missing_values(df):\n",
    "    # Drop rows where ANY value is missing\n",
    "    df_main_rows = df.dropna()\n",
    "    return df_main_rows\n",
    "\n",
    "\n",
    "# Initial load data preprocessing\n",
    "def preprocess_data(df):\n",
    "    # clean the column names\n",
    "    df_clean_cols = cleanColumnNames(df)\n",
    "\n",
    "    # Validate date objects\n",
    "    df_date_processed = validate_format_date_time_objects(df_clean_cols)\n",
    "\n",
    "    # Validate booloean fields\n",
    "    df_bool_processed = validate_boolean_data(df_date_processed)\n",
    "\n",
    "    # Treate missing values\n",
    "    df_clean_missing = drop_missing_values(df_bool_processed)\n",
    "\n",
    "    return df_clean_missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482734eb",
   "metadata": {},
   "source": [
    "### Kafka producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378c262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "\n",
    "def initialise_kafka(bootstrap_server):\n",
    "    # Initialize Kafka producer\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=bootstrap_server,\n",
    "        value_serializer=lambda v: json.dumps(v, default=str).encode('utf-8')  # serialize dict to JSON bytes\n",
    "    )\n",
    "    return producer\n",
    "\n",
    "\n",
    "# format and clean data to kafka friendly format\n",
    "def send_to_kafka(producer, topic, df):\n",
    "    # Convert each row in DataFrame to JSON and send to Kafka\n",
    "    for _, row in df.iterrows():\n",
    "        message = row.to_dict()  # convert row to dict\n",
    "        producer.send(topic, value=message)\n",
    "\n",
    "    producer.flush()  # ensure all messages sent\n",
    "    print('Data sent to Kafka topic successfully')\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ee3fb",
   "metadata": {},
   "source": [
    "### Read the data chunk wise and run the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddbaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_42656\\2597095664.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['datetime'] = pd.to_datetime(df['read_date'], errors='coerce')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_42656\\2597095664.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  s = pd.to_datetime(df['read_date'], errors='coerce')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_42656\\2597095664.py:17: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  df['timestamp_ms'] = (s.view('int64') // 10**6).astype('Int64')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial load: 500 records\n",
      "Data sent to Kafka topic successfully\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "file_path = \"../dataset/Camera_Traffic_Counts_20251113.csv\"\n",
    "initial_load = 500\n",
    "batch_size = 500\n",
    "delay_seconds = 5\n",
    "offset_track =initial_load\n",
    "\n",
    "# Kafka\n",
    "bootstrap_server = 'localhost:9092'\n",
    "topic = 'raw_sensor_data_ingestion'\n",
    "\n",
    "# initialise the kafka\n",
    "producer = initialise_kafka(bootstrap_server)\n",
    "\n",
    "# Create an iterator for reading the CSV in chunks\n",
    "chunk_iter = pd.read_csv(file_path, chunksize=batch_size)\n",
    "\n",
    "# Load the initial 100,000 records in one go (if memory permits)\n",
    "df_initial = pd.read_csv(file_path, nrows=initial_load)\n",
    "processed_data = preprocess_data(df_initial)\n",
    "print(f'Initial load: {len(processed_data)} records')\n",
    "\n",
    "# Initial data load pass to kafka\n",
    "send_to_kafka(producer, topic, processed_data)\n",
    "\n",
    "# Then load 12 records every 5 seconds repeatedly\n",
    "# try:\n",
    "#     while True:\n",
    "\n",
    "#         # Create an iterator for reading the CSV in chunks\n",
    "#         chunk_iter = pd.read_csv(\n",
    "#             file_path,\n",
    "#             chunksize=batch_size,\n",
    "#             skiprows=range(offset_track + 1, offset_track + batch_size)  # skip first initial data set\n",
    "#         )\n",
    "#         print(f'start-index: {offset_track + 1} | end-index: {offset_track + batch_size} ')\n",
    "\n",
    "#         for chunk in chunk_iter:\n",
    "\n",
    "#             # Process the chunk here\n",
    "#             processed_chunk = preprocess_data(chunk)\n",
    "#             print(f'Loaded batch of {len(processed_chunk)} records')\n",
    "\n",
    "#             # Repeatedly send data chunks to kafka\n",
    "#             send_to_kafka(producer, topic, processed_chunk)\n",
    "        \n",
    "#         # Update the offset_tracker\n",
    "#         offset_track = offset_track + batch_size\n",
    "\n",
    "#         break  # Exit the loop\n",
    "#         time.sleep(delay_seconds)\n",
    "# except StopIteration:\n",
    "#     print('End of file reached')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
